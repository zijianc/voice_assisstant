#!/usr/bin/env python3
"""
ä¸¥è°¨æµ‹è¯•æ‰§è¡Œç®¡ç†å™¨
================

ç»Ÿä¸€ç®¡ç†å’Œæ‰§è¡Œæ‰€æœ‰ä¸¥è°¨æµ‹è¯•ï¼Œç¡®ä¿å­¦æœ¯æŠ¥å‘Šçš„æ•°æ®ä¸¥è°¨æ€§

ç”¨æ³•:
    python3 rigorous_test_manager.py --run-all
    python3 rigorous_test_manager.py --wakeword --latency  
    python3 rigorous_test_manager.py --interruption-only
"""

import argparse
import json
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

class RigorousTestManager:
    def __init__(self):
        self.start_time = datetime.now()
        self.test_results = {}
        
        # æ¨èçš„æµ‹è¯•é…ç½®ï¼ˆç¬¦åˆå­¦æœ¯æ ‡å‡†ï¼‰
        self.test_configs = {
            'wakeword': {
                'script': 'rigorous_wakeword_test.py',
                'args': ['--snr-levels', '20', '10', '0', '--samples-per-snr', '30', '--rounds', '1'],
                'description': 'å”¤é†’è¯ç²¾åº¦æµ‹è¯• (æ¯SNRçº§åˆ«30æ ·æœ¬)',
                'expected_duration': '10-15åˆ†é’Ÿ'
            },
            'latency': {
                'script': 'rigorous_latency_test.py', 
                'args': ['--samples', '50', '--with-interruption', '--inter-trial-delay', '2.0'],
                'description': 'ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯• (50æ ·æœ¬å«ä¸­æ–­)',
                'expected_duration': '15-20åˆ†é’Ÿ'
            },
            'interruption': {
                'script': 'rigorous_interruption_test.py',
                'args': ['--samples', '60', '--interruption-times', '0.2', '0.5', '1.0', '2.0'],
                'description': 'ä¸“é—¨ä¸­æ–­å»¶è¿Ÿæµ‹è¯• (60æ ·æœ¬4ä¸ªæ—¶æœº)',
                'expected_duration': '12-18åˆ†é’Ÿ'
            }
        }

    def check_prerequisites(self):
        """æ£€æŸ¥æµ‹è¯•å‰ææ¡ä»¶"""
        print("ğŸ” æ£€æŸ¥æµ‹è¯•å‰ææ¡ä»¶...")
        
        errors = []
        warnings = []
        
        # æ£€æŸ¥æµ‹è¯•è„šæœ¬
        for test_name, config in self.test_configs.items():
            script_path = Path(config['script'])
            if not script_path.exists():
                errors.append(f"ç¼ºå°‘æµ‹è¯•è„šæœ¬: {config['script']}")
            elif not script_path.is_file():
                errors.append(f"æµ‹è¯•è„šæœ¬ä¸æ˜¯æ–‡ä»¶: {config['script']}")
        
        # æ£€æŸ¥Pythonä¾èµ–
        try:
            import numpy
            import statistics
        except ImportError as e:
            errors.append(f"ç¼ºå°‘Pythonä¾èµ–: {e}")
        
        # æ£€æŸ¥scipyï¼ˆå¯é€‰ä½†æ¨èï¼‰
        try:
            import scipy.stats
        except ImportError:
            warnings.append("å»ºè®®å®‰è£…scipyä»¥è·å¾—æ›´å‡†ç¡®çš„ç»Ÿè®¡åˆ†æ: pip install scipy")
        
        # æ£€æŸ¥ROS2ç¯å¢ƒ
        ros2_setup = Path("/opt/ros/humble/setup.bash")
        if not ros2_setup.exists():
            errors.append("ROS2 Humbleç¯å¢ƒæœªæ‰¾åˆ°")
        
        workspace_setup = Path("install/setup.bash")
        if not workspace_setup.exists():
            warnings.append("å·¥ä½œç©ºé—´æœªæ„å»ºï¼Œè¯·å…ˆè¿è¡Œ: colcon build --packages-select my_voice_assistant")
        
        # æŠ¥å‘Šæ£€æŸ¥ç»“æœ
        if errors:
            print("âŒ å‘ç°é”™è¯¯:")
            for error in errors:
                print(f"   â€¢ {error}")
            return False
        
        if warnings:
            print("âš ï¸  è­¦å‘Š:")
            for warning in warnings:
                print(f"   â€¢ {warning}")
        
        print("âœ… å‰ææ¡ä»¶æ£€æŸ¥é€šè¿‡")
        return True

    def run_single_test(self, test_name):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        if test_name not in self.test_configs:
            print(f"âŒ æœªçŸ¥æµ‹è¯•: {test_name}")
            return False
        
        config = self.test_configs[test_name]
        print(f"\nğŸš€ å¼€å§‹ {test_name} æµ‹è¯•")
        print(f"   æè¿°: {config['description']}")
        print(f"   é¢„è®¡è€—æ—¶: {config['expected_duration']}")
        
        # æ„å»ºå‘½ä»¤
        cmd = ['python3', config['script']] + config['args']
        print(f"   å‘½ä»¤: {' '.join(cmd)}")
        
        # æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)  # 30åˆ†é’Ÿè¶…æ—¶
            duration = time.time() - start_time
            
            if result.returncode == 0:
                print(f"âœ… {test_name} æµ‹è¯•å®Œæˆ (è€—æ—¶: {duration:.1f}s)")
                self.test_results[test_name] = {
                    'status': 'success',
                    'duration': duration,
                    'stdout': result.stdout,
                    'stderr': result.stderr
                }
                return True
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥ (è¿”å›ç : {result.returncode})")
                print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                self.test_results[test_name] = {
                    'status': 'failed',
                    'duration': duration,
                    'returncode': result.returncode,
                    'stdout': result.stdout,
                    'stderr': result.stderr
                }
                return False
                
        except subprocess.TimeoutExpired:
            print(f"â° {test_name} æµ‹è¯•è¶…æ—¶")
            self.test_results[test_name] = {
                'status': 'timeout',
                'duration': time.time() - start_time
            }
            return False
        except Exception as e:
            print(f"ğŸ’¥ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            self.test_results[test_name] = {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            return False

    def run_tests(self, test_names):
        """è¿è¡ŒæŒ‡å®šçš„æµ‹è¯•"""
        if not self.check_prerequisites():
            print("âŒ å‰ææ¡ä»¶ä¸æ»¡è¶³ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
            return False
        
        print(f"\nğŸ“‹ è®¡åˆ’è¿è¡Œæµ‹è¯•: {', '.join(test_names)}")
        total_estimated_time = sum([15 for _ in test_names])  # ç²—ç•¥ä¼°è®¡æ¯ä¸ªæµ‹è¯•15åˆ†é’Ÿ
        print(f"ğŸ“… é¢„è®¡æ€»è€—æ—¶: {total_estimated_time}åˆ†é’Ÿ")
        
        # ç¡®è®¤æ‰§è¡Œ
        response = input("\nç»§ç»­æ‰§è¡Œå—? [y/N]: ")
        if response.lower() not in ['y', 'yes']:
            print("âš ï¸  ç”¨æˆ·å–æ¶ˆæµ‹è¯•")
            return False
        
        successful_tests = []
        failed_tests = []
        
        for i, test_name in enumerate(test_names, 1):
            print(f"\n{'='*60}")
            print(f"æ‰§è¡Œæµ‹è¯• {i}/{len(test_names)}: {test_name.upper()}")
            print(f"{'='*60}")
            
            if self.run_single_test(test_name):
                successful_tests.append(test_name)
            else:
                failed_tests.append(test_name)
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                if i < len(test_names):
                    response = input(f"\n{test_name} æµ‹è¯•å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œå‰©ä½™æµ‹è¯•å—? [y/N]: ")
                    if response.lower() not in ['y', 'yes']:
                        print("âš ï¸  ç”¨æˆ·ä¸­æ­¢åç»­æµ‹è¯•")
                        break
        
        # æµ‹è¯•æ€»ç»“
        self.print_summary(successful_tests, failed_tests)
        self.save_execution_report()
        
        return len(failed_tests) == 0

    def print_summary(self, successful_tests, failed_tests):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print(f"\n{'='*60}")
        print("ğŸ¯ æµ‹è¯•æ‰§è¡Œæ€»ç»“")
        print(f"{'='*60}")
        
        total_duration = sum([r.get('duration', 0) for r in self.test_results.values()])
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.1f}s ({total_duration/60:.1f}åˆ†é’Ÿ)")
        
        if successful_tests:
            print(f"\nâœ… æˆåŠŸæµ‹è¯• ({len(successful_tests)}):")
            for test in successful_tests:
                duration = self.test_results[test].get('duration', 0)
                print(f"   â€¢ {test}: {duration:.1f}s")
        
        if failed_tests:
            print(f"\nâŒ å¤±è´¥æµ‹è¯• ({len(failed_tests)}):")
            for test in failed_tests:
                status = self.test_results[test].get('status', 'unknown')
                print(f"   â€¢ {test}: {status}")
        
        print(f"\nğŸ“Š æˆåŠŸç‡: {len(successful_tests)}/{len(successful_tests)+len(failed_tests)} ({len(successful_tests)/(len(successful_tests)+len(failed_tests))*100:.1f}%)")

    def save_execution_report(self):
        """ä¿å­˜æ‰§è¡ŒæŠ¥å‘Š"""
        timestamp = self.start_time.strftime('%Y%m%d_%H%M%S')
        report_file = f"rigorous_test_execution_{timestamp}.json"
        
        report = {
            'execution_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_duration': sum([r.get('duration', 0) for r in self.test_results.values()]),
                'test_configs': self.test_configs
            },
            'results': self.test_results,
            'summary': {
                'total_tests': len(self.test_results),
                'successful_tests': len([r for r in self.test_results.values() if r.get('status') == 'success']),
                'failed_tests': len([r for r in self.test_results.values() if r.get('status') != 'success'])
            }
        }
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nğŸ“ æ‰§è¡ŒæŠ¥å‘Šä¿å­˜ä¸º: {report_file}")

    def list_available_tests(self):
        """åˆ—å‡ºå¯ç”¨çš„æµ‹è¯•"""
        print("ğŸ“‹ å¯ç”¨çš„ä¸¥è°¨æµ‹è¯•:")
        for test_name, config in self.test_configs.items():
            print(f"\nğŸ§ª {test_name.upper()}")
            print(f"   æè¿°: {config['description']}")
            print(f"   é¢„è®¡è€—æ—¶: {config['expected_duration']}")
            print(f"   è„šæœ¬: {config['script']}")
            print(f"   å‚æ•°: {' '.join(config['args'])}")

    def check_existing_results(self):
        """æ£€æŸ¥å·²æœ‰çš„æµ‹è¯•ç»“æœ"""
        print("ğŸ” æ£€æŸ¥å·²æœ‰æµ‹è¯•ç»“æœ...")
        
        result_patterns = {
            'wakeword': 'rigorous_wakeword_*.json',
            'latency': 'rigorous_latency_*.json', 
            'interruption': 'rigorous_interruption_*.json'
        }
        
        found_results = {}
        for test_type, pattern in result_patterns.items():
            files = list(Path('.').glob(pattern))
            if files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                latest_file = max(files, key=lambda f: f.stat().st_mtime)
                found_results[test_type] = {
                    'file': str(latest_file),
                    'modified': datetime.fromtimestamp(latest_file.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                }
        
        if found_results:
            print("ğŸ“Š å‘ç°å·²æœ‰æµ‹è¯•ç»“æœ:")
            for test_type, info in found_results.items():
                print(f"   â€¢ {test_type}: {info['file']} (ä¿®æ”¹äº {info['modified']})")
            print("ğŸ’¡ å¦‚éœ€é‡æ–°æµ‹è¯•ï¼Œå»ºè®®å…ˆå¤‡ä»½ç°æœ‰ç»“æœ")
        else:
            print("ğŸ“­ æœªå‘ç°å·²æœ‰æµ‹è¯•ç»“æœ")
        
        return found_results

def main():
    parser = argparse.ArgumentParser(description='ä¸¥è°¨æµ‹è¯•æ‰§è¡Œç®¡ç†å™¨')
    parser.add_argument('--run-all', action='store_true',
                       help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    parser.add_argument('--wakeword', action='store_true',
                       help='è¿è¡Œå”¤é†’è¯æµ‹è¯•')
    parser.add_argument('--latency', action='store_true',
                       help='è¿è¡Œå»¶è¿Ÿæµ‹è¯•')
    parser.add_argument('--interruption', action='store_true',
                       help='è¿è¡Œä¸­æ–­æµ‹è¯•')
    parser.add_argument('--interruption-only', action='store_true',
                       help='ä»…è¿è¡Œä¸­æ–­æµ‹è¯•')
    parser.add_argument('--list', action='store_true',
                       help='åˆ—å‡ºå¯ç”¨æµ‹è¯•')
    parser.add_argument('--check-results', action='store_true',
                       help='æ£€æŸ¥å·²æœ‰ç»“æœ')
    
    args = parser.parse_args()
    
    manager = RigorousTestManager()
    
    if args.list:
        manager.list_available_tests()
        return
    
    if args.check_results:
        manager.check_existing_results()
        return
    
    # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•
    tests_to_run = []
    
    if args.run_all:
        tests_to_run = ['wakeword', 'latency', 'interruption']
    else:
        if args.wakeword:
            tests_to_run.append('wakeword')
        if args.latency:
            tests_to_run.append('latency')
        if args.interruption or args.interruption_only:
            tests_to_run.append('interruption')
    
    if not tests_to_run:
        print("â“ æœªæŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•")
        print("ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æµ‹è¯•ï¼Œæˆ– --help æŸ¥çœ‹å¸®åŠ©")
        return
    
    # æ£€æŸ¥å·²æœ‰ç»“æœ
    existing_results = manager.check_existing_results()
    if existing_results:
        overlapping = set(tests_to_run) & set(existing_results.keys())
        if overlapping:
            print(f"\nâš ï¸  è­¦å‘Š: ä»¥ä¸‹æµ‹è¯•å·²æœ‰ç»“æœæ–‡ä»¶: {', '.join(overlapping)}")
            response = input("ç»§ç»­æ‰§è¡Œä¼šç”Ÿæˆæ–°çš„ç»“æœæ–‡ä»¶ï¼Œæ˜¯å¦ç»§ç»­? [y/N]: ")
            if response.lower() not in ['y', 'yes']:
                print("âš ï¸  ç”¨æˆ·å–æ¶ˆæµ‹è¯•")
                return
    
    # æ‰§è¡Œæµ‹è¯•
    print(f"\nğŸ¯ å‡†å¤‡æ‰§è¡Œä¸¥è°¨æµ‹è¯•ä»¥ç¡®ä¿å­¦æœ¯æŠ¥å‘Šæ•°æ®ä¸¥è°¨æ€§")
    success = manager.run_tests(tests_to_run)
    
    if success:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“š ç”Ÿæˆçš„æ•°æ®å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œå¯ç”¨äºå­¦æœ¯æŠ¥å‘Š")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡æ–°è¿è¡Œ")
        sys.exit(1)

if __name__ == "__main__":
    main()