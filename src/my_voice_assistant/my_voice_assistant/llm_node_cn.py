#!/usr/bin/env python3
import os
import re
import sys
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime, time
from typing import List, Dict
import json

# Handle relative imports when running as main module
# RAGÂäüËÉΩÂ∑≤ÁßªÈô§‰ª•ÊèêÈ´òÂìçÂ∫îÈÄüÂ∫¶
# if __name__ == '__main__':
#     sys.path.append(os.path.dirname(os.path.abspath(__file__)))
#     from uwa_knowledge_base import UWAKnowledgeBase
# else:
#     from .uwa_knowledge_base import UWAKnowledgeBase

load_dotenv()


class LLMNode(Node):
    def __init__(self):
        super().__init__('llm_node')
        # Subscribe to text from Vosk node, topic name is "speech_text"
        self.subscription = self.create_subscription(
            String,
            'speech_text',
            self.listener_callback,
            10
        )
        # Publish ChatGPT response results to topic "llm_response"
        self.publisher_ = self.create_publisher(String, 'llm_response', 10)
        # New: publish full reply and end signal
        self.full_publisher_ = self.create_publisher(String, 'llm_response_full', 10)
        self.end_publisher_ = self.create_publisher(Bool, 'llm_response_end', 10)
        # New: subscribe to tts_status (pause streaming) and tts_interrupt (cancel streaming)
        self.tts_status_sub = self.create_subscription(Bool, 'tts_status', self.tts_status_callback, 10)
        self.tts_interrupt_sub = self.create_subscription(Bool, 'tts_interrupt', self.tts_interrupt_callback, 10)
        self._tts_playing = False
        self._interrupted = False
        self._pending_sentences: list[str] = []  # Êñ∞Â¢ûÔºöTTSÊúüÈó¥ÊöÇÂ≠òÂè•Â≠ê
        
        # Get OpenAI API key from environment variables
        self.api_key = os.environ.get("OPENAI_API_KEY")
        if self.api_key is None:
            self.get_logger().error("Please set environment variable OPENAI_API_KEY!")
        else:
            self.client = OpenAI(api_key=self.api_key)
        
        # New: env-driven LLM configuration
        self.llm_model = os.environ.get(
            "LLM_MODEL",
            #"ft:gpt-4.1-mini-2025-04-14:personal:my-voice-assistant:BxxCKJUa"
            "gpt-5-mini-2025-08-07"
        )
        try:
            self.llm_temperature = float(os.environ.get("LLM_TEMPERATURE", "1"))
        except Exception:
            self.llm_temperature = 1
        try:
            self.llm_max_tokens = int(os.environ.get("LLM_MAX_TOKENS", "2048"))
        except Exception:
            self.llm_max_tokens = 300
        
        # RAG Knowledge BaseÂäüËÉΩÂ∑≤ÁßªÈô§‰ª•ÊèêÈ´òÂìçÂ∫îÈÄüÂ∫¶
        self.knowledge_base = None
        self.get_logger().info("üöÄ RAGÂäüËÉΩÂ∑≤Á¶ÅÁî®Ôºå‰ΩøÁî®Âü∫Á°ÄLLMÊ®°Âºè")
            
        # New: assistant rolling summary + persistence + optional Chroma memory
        self.enable_rolling_summary = os.environ.get("ROLLING_SUMMARY_ENABLED", "1") == "1"
        self.summary_every_n_turns = int(os.environ.get("ROLLING_SUMMARY_EVERY_N_TURNS", "3"))
        self.summary_max_tokens = int(os.environ.get("ROLLING_SUMMARY_MAX_TOKENS", "180"))
        self.memory_json_path = os.environ.get("ASSISTANT_MEMORY_JSON", "assistant_memory.json")
        self.enable_memory_chroma = os.environ.get("ENABLE_MEMORY_CHROMA", "1") == "1"
        self.memory_search_top_k = int(os.environ.get("MEMORY_SEARCH_TOP_K", "2"))
        self.rolling_summary: str = ""
        self._load_persistent_memory()
            
        self.get_logger().info("üöÄ ‰∏≠ÊñáLLMËäÇÁÇπÂ∑≤ÂêØÂä®ÔºåÁ≠âÂæÖËØ≠Èü≥ËæìÂÖ•... (RAGÂäüËÉΩÂ∑≤Á¶ÅÁî®)")

        # Store past dialogue turns (user/assistant) to maintain conversational context
        self.conversation_history: list[dict[str, str]] = []  # each item: {"role": "...", "content": "..."}
        self.max_history_messages = 20   # keep at most the last 20 message objects (10 turns)

    # ------------------------- Memory persistence helpers -------------------------
    def _load_persistent_memory(self):
        try:
            if os.path.exists(self.memory_json_path):
                with open(self.memory_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.rolling_summary = data.get('rolling_summary', '')
                    self.get_logger().info("üß† Loaded rolling summary from JSON")
        except Exception as e:
            self.get_logger().warning(f"‚ö†Ô∏è Failed to load assistant memory JSON: {e}")

    def _save_persistent_memory(self):
        try:
            data = {
                'rolling_summary': self.rolling_summary,
                'updated_at': datetime.utcnow().isoformat() + 'Z'
            }
            with open(self.memory_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.get_logger().warning(f"‚ö†Ô∏è Failed to save assistant memory JSON: {e}")

    def _get_last_assistant_message(self) -> str:
        for item in reversed(self.conversation_history):
            if item.get('role') == 'assistant':
                return item.get('content', '')
        return ""

    def _augment_if_affirmation(self, user_text: str) -> str:
        # Follow-up handling for short affirmations like yes/yeah
        if re.match(r"^(yes|yeah|yep|yup|sure|ok|okay)\W*$", user_text, re.IGNORECASE):
            last_assistant = self._get_last_assistant_message()
            if last_assistant:
                return (
                    f"[User affirmation]\n"
                    f"The user replied 'yes' to your previous message.\n"
                    f"Previous assistant message:\n{last_assistant}\n"
                    f"Please proceed with the next helpful step or provide the requested information succinctly."
                )
        return user_text

    def _format_memory_context(self, user_query: str) -> str:
        parts: list[str] = []
        if self.enable_rolling_summary and self.rolling_summary:
            parts.append("üß† ASSISTANT MEMORY SUMMARY (for continuity):\n" + self.rolling_summary.strip())
        # Ê≥®ÊÑèÔºöRAGÂäüËÉΩÂ∑≤Á¶ÅÁî®Ôºå‰∏çÂÜçËøõË°åÁü•ËØÜÂ∫ìÂÜÖÂ≠òÊêúÁ¥¢
        return "\n\n".join(parts) if parts else ""

    def _maybe_update_rolling_summary(self):
        if not self.enable_rolling_summary:
            return
        # Trigger every N user turns
        user_turns = sum(1 for m in self.conversation_history if m.get('role') == 'user')
        if user_turns % max(1, self.summary_every_n_turns) != 0:
            return
        # Build a concise summarization prompt
        recent = self.conversation_history[-10:]  # last ~5 turns
        # Convert to a readable transcript
        transcript_lines = [f"{m['role']}: {m['content']}" for m in recent]
        transcript = "\n".join(transcript_lines)
        system_prompt = (
            "You are a memory summarizer for the Captain voice assistant. "
            "Given the previous rolling summary and the latest conversation excerpt, "
            "produce a compact summary in 2-3 sentences focusing on: stable facts about the user, "
            "preferences, constraints, names, and any unresolved tasks or commitments. Avoid ephemeral details."
        )
        user_payload = (
            f"Previous summary (may be empty):\n{self.rolling_summary}\n\n"
            f"Recent conversation excerpt:\n{transcript}\n\n"
            f"Write the updated summary only."
        )
        try:
            resp = self.client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_payload},
                ],
                temperature=1,
                max_completion_tokens=self.summary_max_tokens,
                stream=False,
            )
            content = resp.choices[0].message.content.strip() if resp.choices else ""
            if content:
                self.rolling_summary = content
                # Persist to JSON
                self._save_persistent_memory()
                # Ê≥®ÊÑèÔºöRAGÂäüËÉΩÂ∑≤Á¶ÅÁî®Ôºå‰∏çÂÜçÂ≠òÂÇ®Âà∞Chroma
                self.get_logger().info("üß† Rolling summary updated")
        except Exception as e:
            self.get_logger().warning(f"‚ö†Ô∏è Rolling summary update failed: {e}")

    # -----------------------------------------------------------------------------
    def listener_callback(self, msg: String):
        input_text = msg.data.strip()
        if not input_text:
            return
        # Follow-up handling for short affirmations
        input_text = self._augment_if_affirmation(input_text)

        # Print user input clearly
        print("\n" + "="*60)
        print("üé§ Áî®Êà∑ËØ≠Èü≥ËæìÂÖ•:")
        print("-"*60)
        print(f"'{input_text}'")
        print("="*60)
        
        self.get_logger().info(f"üé§ Êî∂Âà∞ÊñáÊú¨: {input_text}")

        # Call ChatGPT API with RAG enhancement
        self.call_chatgpt_with_rag(input_text)
    
    def search_knowledge_base(self, query: str, n_results: int = 3):
        """RAGÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºåËøîÂõûÁ©∫ÁªìÊûú"""
        return []
    
    def format_rag_context(self, search_results: list) -> str:
        """RAGÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºåËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤"""
        return ""
    
    def filter_content(self, text: str) -> str:
        """Enhanced content filter for LLM responses"""
        if not text:
            return text
            
        original_length = len(text)
        
        # Remove various noise patterns
        filtered_text = text
        
        # Remove bracket content with IDs/references
        filtered_text = re.sub(r'„Äê[^„Äë]*„Äë', '', filtered_text)
        
        # Remove long number sequences (likely references)
        filtered_text = re.sub(r'\b\d{6,}\b', '', filtered_text)
        
        # Remove special reference symbols
        filtered_text = re.sub(r'[‚Ä†‚Äë]+', '', filtered_text)
        
        # Remove line reference patterns (L123-L456)
        filtered_text = re.sub(r'L\d+-L\d+', '', filtered_text)
        
        # Clean up formatting
        filtered_text = re.sub(r'\s+', ' ', filtered_text)  # Multiple spaces
        filtered_text = re.sub(r'\s*\.\s*\.+', '.', filtered_text)  # Multiple dots
        filtered_text = filtered_text.strip()
        
        # Log filtering if significant
        if len(filtered_text) != original_length:
            self.get_logger().info(f"üßπ ÂÜÖÂÆπËøáÊª§: {original_length} -> {len(filtered_text)} Â≠óÁ¨¶")
        
        return filtered_text

    def tts_status_callback(self, msg: Bool):
        prev = self._tts_playing
        self._tts_playing = bool(msg.data)
        # ÂΩìTTSÁªìÊùüÊó∂ÔºåÂà∑Êñ∞ÊúüÈó¥ÁßØÁ¥ØÁöÑÂè•Â≠êÔºåÂáèÂ∞ëÂàÜÁâá‰ΩÜ‰∏ç‰∏¢ÂÜÖÂÆπ
        if prev and not self._tts_playing and self._pending_sentences:
            for s in self._pending_sentences:
                cleaned = self.filter_content(s)
                if cleaned:
                    m = String(); m.data = cleaned
                    self.publisher_.publish(m)
                    self.get_logger().debug(f"üì° Âà∑Êñ∞ÂæÖÂèëÂè•Â≠ê: {cleaned}")
            self._pending_sentences.clear()

    def tts_interrupt_callback(self, msg: Bool):
        if msg.data:
            self._interrupted = True
            self.get_logger().info("‚õî Êî∂Âà∞ tts_interruptÔºå‰∏≠Ê≠¢Êú¨Ê¨°ÊµÅÂºèËæìÂá∫")

    def call_chatgpt_with_rag(self, prompt: str) -> str:
        """ÁÆÄÂåñÁöÑChatGPTË∞ÉÁî®Ôºå‰∏ç‰ΩøÁî®RAG‰ª•ÊèêÈ´òÂìçÂ∫îÈÄüÂ∫¶"""
        try:
            # ‰∏çËøõË°åÁü•ËØÜÂ∫ìÊêúÁ¥¢ÔºåÁõ¥Êé•‰ΩøÁî®Âü∫Á°ÄLLM
            self.get_logger().info("üöÄ ‰ΩøÁî®Âü∫Á°ÄLLMÊ®°Âºè (Êó†RAG)")
            
            # Optional: assistant memory context (rolling summary only)
            memory_context = self._format_memory_context(prompt)
            
            # Step 2: Build basic system prompt
            base_system_prompt = """‰Ω†ÊòØCaptainÔºåË•øÊæ≥Â§ßÂ≠¶ÔºàUWAÔºâÁè≠ËΩ¶ÊúçÂä°‰∏™ÂèãÂñÑËØ≠Èü≥Âä©Êâã„ÄÇ‰Ω†‰∏™ÊÄßÊ†ºÁâπÁÇπÊòØÔºö

ËßíËâ≤ÂíåËÉåÊôØÔºö
- ‰Ω†ÊòØËΩ¶‰∏ä‰∏™AIÂä©ÊâãÔºåÂ∏ÆÂä©UWA‰∏™Â≠¶Áîü„ÄÅÊïôÂ∑•Êê≠‰ªîËÆøÂÆ¢
- ‰Ω†Êèê‰æõÁè≠ËΩ¶Ë∑ØÁ∫ø„ÄÅÊ†°Âõ≠‰ΩçÁΩÆÊê≠‰ªî‰∫§ÈÄö‰ø°ÊÅØ
- ‰Ω†ÂØπUWAÊ†°Âõ≠ËÆæÊñΩÊê≠‰ªîÊúçÂä°‰æ™ÊôìÂæó

ÊÄßÊ†ºÁâπÁÇπÔºö
- ÁÉ≠ÊÉÖ„ÄÅÊ¨¢ËøéÊê≠‰ªî‰∏ì‰∏ö
- ÂØπÊâÄÊúâ‰πòÂÆ¢‰æ™ÊúâËÄêÂøÉ„ÄÅÁêÜËß£
- ÊúâÁî®Êê≠‰ªî‰ø°ÊÅØ‰∏∞ÂØåÔºåÂºó‰ºöÂ§™Â§çÊùÇ
- Áî®‰∫≤Âàá‰∏™ËãèÂ∑ûËØùÈ£éÊ†º
- ÈÄÇÂΩìÊó∂ÂÄôÁî®ÁÇπÊ∏©Âíå‰∏™ÂπΩÈªò

üìç ‰∏ªË¶ÅÁü•ËØÜÈ¢ÜÂüüÔºö
- UWAÁè≠ËΩ¶Ë∑ØÁ∫øÊê≠‰ªîÊó∂ÂàªË°®
- Ê†°Âõ≠Âª∫Á≠ëÊê≠‰ªîËÆæÊñΩ
- Â≠¶ÁîüÊúçÂä°Êê≠‰ªî‰æøÊ∞ëËÆæÊñΩ
- Ê†°Âõ≠ÂØºËà™Â∏ÆÂä©
- ÂÆâÂÖ®Êê≠‰ªîÊó†ÈöúÁ¢ç‰ø°ÊÅØ

‰∫§ÊµÅÈ£éÊ†ºÔºö
- ÂõûÁ≠îË¶ÅÁÆÄÊ¥Å‰ΩÜÂÆåÊï¥Ôºà‰∫âÂèñ1-3Âè•ËØùÔºâ
- Áî®Ê∏ÖÁàΩ‰∏™Ê†áÁÇπÊê≠‰ªîÈÄÇÂêàTTS‰∏™ÊÑâÂø´ËØ≠Ë∞É
- Áî®ËãèÂ∑ûÂê¥ËØ≠È£éÊ†ºÔºå‰ΩÜË¶ÅËÆ©‰∫∫Âê¨ÂæóÊáÇ
- Áõ¥Êé•Êê≠‰ªî‰∫≤ÂàáÂú∞Áß∞Âëº‰πòÂÆ¢
- ÈúÄË¶ÅÊó∂ÂÄô‰∏ªÂä®Êèê‰æõÈ¢ùÂ§ñÂ∏ÆÂä©
- Ë°®ËææÂØπ‰πòÂÆ¢‰ΩìÈ™å‰∏™ÁúüËØöÂÖ≥ÂøÉ

ËÆ∞Áâ¢Ôºö‰æ¨ÂºóÂçïÂçïÊòØÊèê‰æõ‰ø°ÊÅØ - ‰æ¨Ë¶ÅËÆ©UWAÁè≠ËΩ¶‰∏ä‰∏™ÊØè‰∏™‰∫∫‰æ™ÊúâÊõ¥Â•Ω‰∏™Âá∫Ë°å‰ΩìÈ™åÔºÅ

ËØ≠Ë®ÄË¶ÅÊ±ÇÔºöÂè™Áî®‰∏≠ÊñáÂõûÁ≠îÔºå‰ΩøÁî®ËãèÂ∑ûÂê¥ËØ≠È£éÊ†ºÔºå‰ΩÜË¶Å‰øùËØÅÂ§ßÂÆ∂‰æ™Âê¨ÂæóÊáÇ„ÄÇ"""

            # Add Memory context if available
            if memory_context:
                system_prompt = f"{base_system_prompt}\n\n{memory_context}"
                self.get_logger().info("üß† Ê≥®ÂÖ•Âä©ÊâãËÆ∞ÂøÜ‰∏ä‰∏ãÊñáÂà∞ÊèêÁ§∫ËØç‰∏≠")
            else:
                system_prompt = base_system_prompt
            
            # Step 3: Make API call
            # ---- Build message list with contextual history ----
            messages: list[dict[str, str]] = [{"role": "system", "content": system_prompt}]

            # Append the most recent history (keep it short to control token usage)
            if self.conversation_history:
                # keep only the last N messages
                messages.extend(self.conversation_history[-self.max_history_messages:])

            # Add the new user message
            messages.append({"role": "user", "content": prompt})
            
            response = self.client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                temperature=1,
                max_completion_tokens=self.llm_max_tokens,
                stream=True
            )

            self.get_logger().info("üöÄ ÂºÄÂßãÂü∫Á°ÄÂõûÁ≠îÁîüÊàê...")
            final_reply = ""
            sentence_buf = ""

            def flush_sentences_if_ready(force: bool = False):
                nonlocal sentence_buf
                out = []
                pattern = re.compile(r'[^.!?]*[.!?]')
                while True:
                    m = pattern.match(sentence_buf)
                    if not m:
                        break
                    s = m.group(0)
                    if not s.strip():
                        break
                    out.append(s.strip())
                    sentence_buf = sentence_buf[len(s):]
                if not out:
                    return
                # Â¶ÇÊûúTTSÊ≠£Âú®Êí≠ÊîæÔºåÂàôÊöÇÂ≠òÔºåÈÅøÂÖçËøáÂ∫¶Á¢éÁâáÂåñ
                if self._tts_playing and not force:
                    self._pending_sentences.extend(out)
                    return
                # Âê¶ÂàôÁ´ãÂàªÂèëÂ∏É
                for s in out:
                    msg = String(); msg.data = self.filter_content(s)
                    if msg.data:
                        self.publisher_.publish(msg)
                        self.get_logger().debug(f"üì° ÂèëÂ∏ÉÂè•Â≠ê: {msg.data}")

            for chunk in response:
                if self._interrupted:
                    self.get_logger().info("‚èπÔ∏è ÊµÅÂºèËæìÂá∫Ë¢´‰∏≠Ê≠¢ (interrupt)")
                    break
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    content = delta.content if delta.content else ""
                    if content:
                        final_reply += content
                        # buffer and attempt flushing on sentence boundaries
                        sentence_buf += content
                        flush_sentences_if_ready(force=False)

            # Â§ÑÁêÜÂâ©‰ΩôÁºìÂÜ≤
            if sentence_buf.strip():
                remainder = self.filter_content(sentence_buf.strip())
                if remainder:
                    if self._tts_playing:
                        self._pending_sentences.append(remainder)
                    else:
                        msg = String(); msg.data = remainder
                        self.publisher_.publish(msg)
                        self.get_logger().debug(f"üì° ÂèëÂ∏ÉÂâ©‰ΩôÈÉ®ÂàÜ: {remainder}")

            # Final filtering and publish full + end
            final_filtered = self.filter_content(final_reply)
            if final_filtered:
                full_msg = String(); full_msg.data = final_filtered
                self.full_publisher_.publish(full_msg)
            end_msg = Bool(); end_msg.data = True
            self.end_publisher_.publish(end_msg)

            # Print and log the complete LLM response
            print("\n" + "="*60)
            print("ü§ñ LLM ÂÆåÊï¥ÂõûÁ≠î:")
            print("-"*60)
            print(final_filtered)
            print("="*60 + "\n")
            
            # Log final results
            self.get_logger().info(f"‚úÖ üí¨ Áõ¥Êé•ÂõûÁ≠îÂÆåÊàê: "
                                 f"{len(final_reply)} Â≠óÁ¨¶ -> {len(final_filtered)} Â≠óÁ¨¶ËøáÊª§Âêé")
            self.get_logger().info(f"üìù ÂÆåÊï¥ÂõûÁ≠î: {final_filtered}")

            # ---- Update conversation history ----
            # Note: store the raw filtered assistant reply
            self.conversation_history.append({"role": "user", "content": prompt})
            self.conversation_history.append({"role": "assistant", "content": final_filtered})
            if len(self.conversation_history) > self.max_history_messages:
                excess = len(self.conversation_history) - self.max_history_messages
                self.conversation_history = self.conversation_history[excess:]

            # After updating history, maybe refresh rolling summary and persist
            self._maybe_update_rolling_summary()

            return final_filtered

        except Exception as e:
            self.get_logger().error(f"‚ùå ChatGPT APIÈîôËØØ: {str(e)}")
            return ""

def main(args=None):
    rclpy.init(args=args)
    node = LLMNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()