import os
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool
import pyaudio
import queue
import threading
import time
import difflib
import wave
import tempfile
import openai
import tenacity
from dotenv import load_dotenv
import numpy as np
import collections
from concurrent.futures import ThreadPoolExecutor

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# -----------------------------------------------------------------------------
# Audio configuration
SAMPLE_RATE = 24000          # Whisper é»˜è®¤ 24 kHz
CHUNK_SIZE = 1024           # éŸ³é¢‘å—å¤§å°
MODEL_NAME = os.getenv("OPENAI_STT_MODEL", "whisper-1")

# VAD é…ç½® - è°ƒæ•´è¿™äº›å‚æ•°æ¥æ”¹å–„å”¤é†’è¯æ£€æµ‹
VAD_THRESHOLD = 0.015       # é™ä½é˜ˆå€¼ï¼Œä½¿VADæ›´æ•æ„Ÿ
SILENCE_DURATION = 2.0      # é™éŸ³æŒç»­æ—¶é—´ (ç§’)
MIN_SPEECH_DURATION = 0.2   # é™ä½æœ€å°è¯­éŸ³æŒç»­æ—¶é—´ï¼Œæ›´å¿«å“åº”
BUFFER_HISTORY = 1.5        # å¢åŠ å‰ç¼“å†²åŒºæ—¶é—´ï¼Œç¡®ä¿æ•è·å®Œæ•´å”¤é†’è¯
# -----------------------------------------------------------------------------

class OpenAISTTNodeWithVAD(Node):
    def __init__(self):
        super().__init__('openai_stt_node_with_vad')
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        self.openai_client = openai.OpenAI(api_key=api_key)
        
        # åˆ›å»ºå‘å¸ƒè€…ï¼Œå‘å¸ƒè¯†åˆ«ç»“æœåˆ°è¯é¢˜ 'speech_text'
        self.publisher_ = self.create_publisher(String, 'speech_text', 10)
        # è®¢é˜… TTS çŠ¶æ€æ¶ˆæ¯
        self.tts_status_sub = self.create_subscription(Bool, 'tts_status', self.tts_status_callback, 10)
        self.listening = True
        # TTS æ‰“æ–­ç›¸å…³
        self.tts_active = False                       # å½“å‰æ˜¯å¦åœ¨æ’­æŠ¥
        self.interrupt_pub = self.create_publisher(Bool, 'tts_interrupt', 10)

        # è¯­è¨€å‚æ•°ï¼šç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        self.declare_parameter('language', '')
        self.language = self.get_parameter('language').value

        # çº¿ç¨‹æ± ï¼šç»Ÿä¸€é™æµè¯­éŸ³è½¬å†™è¯·æ±‚
        self.transcribe_executor = ThreadPoolExecutor(max_workers=2)

        # å…è®¸ç”¨æˆ·è°ƒæ•´VADå‚æ•°
        self.declare_parameter('vad_threshold', VAD_THRESHOLD)
        self.declare_parameter('silence_duration', SILENCE_DURATION)
        self.declare_parameter('min_speech_duration', MIN_SPEECH_DURATION)
        self.declare_parameter('buffer_history', BUFFER_HISTORY)
        
        # è·å–å‚æ•°
        self.vad_threshold = self.get_parameter('vad_threshold').value
        self.silence_duration = self.get_parameter('silence_duration').value
        self.min_speech_duration = self.get_parameter('min_speech_duration').value
        self.buffer_history = self.get_parameter('buffer_history').value

        # åˆå§‹åŒ–éŸ³é¢‘è¾“å…¥
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=CHUNK_SIZE
        )
        self.stream.start_stream()
        self.get_logger().info("å¼€å§‹ç›‘å¬éº¦å…‹é£ (å¸¦VAD)...")

        # è®¾ç½®å”¤é†’è¯
        self.wake_words = ["hi captain", "hey captain", "hello captain"]

        # VAD ç›¸å…³å˜é‡
        self.vad_state = "silence"  # "silence", "speech", "processing"
        self.speech_buffer = collections.deque(maxlen=int(SAMPLE_RATE * self.buffer_history / CHUNK_SIZE))
        self.current_speech = bytearray()
        self.silence_counter = 0
        self.speech_counter = 0
        self.last_speech_time = 0
        
        # èƒ½é‡å†å²ï¼Œç”¨äºåŠ¨æ€é˜ˆå€¼è°ƒæ•´
        self.energy_history = collections.deque(maxlen=50)  # å­˜å‚¨æœ€è¿‘50ä¸ªèƒ½é‡å€¼
        self.background_energy = 0.01  # åˆå§‹èƒŒæ™¯èƒ½é‡ä¼°è®¡å€¼

        # åˆ›å»ºéŸ³é¢‘å¤„ç†çº¿ç¨‹
        self.audio_thread = threading.Thread(target=self.audio_processing_thread, daemon=True)
        self.audio_thread.start()

        self.get_logger().info(f"VAD é…ç½®: é˜ˆå€¼={self.vad_threshold}, é™éŸ³æ£€æµ‹={self.silence_duration}s, " +
                               f"å‰ç¼“å†²åŒº={self.buffer_history}s, æœ€å°è¯­éŸ³={self.min_speech_duration}s")

    def calculate_rms(self, audio_data):
        """è®¡ç®—éŸ³é¢‘RMS (å‡æ–¹æ ¹) ç”¨äºVAD"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        # è®¡ç®—RMS
        rms = np.sqrt(np.mean(audio_np.astype(np.float64) ** 2))
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        return rms / 32768.0

    def update_background_energy(self, rms):
        """æ›´æ–°èƒŒæ™¯å™ªéŸ³èƒ½é‡ä¼°è®¡ï¼ˆæŒ‡æ•°å¹³æ»‘ï¼‰"""
        # Î± = 0.95ï¼Œå¹³æ»‘ç³»æ•°ï¼›å¯è°ƒ
        self.background_energy = 0.95 * self.background_energy + 0.05 * rms
        # å£°éŸ³æ´»åŠ¨é˜ˆå€¼ä¸ºèƒŒæ™¯èƒ½é‡çš„ 2.5 å€æˆ–æœ€å° vad_threshold
        return max(self.background_energy * 2.5, self.vad_threshold)

    def audio_processing_thread(self):
        """éŸ³é¢‘å¤„ç†çº¿ç¨‹ï¼ŒåŒ…å«VADé€»è¾‘"""
        continuous_listening = True  # è¿ç»­ç›‘å¬æ¨¡å¼ï¼Œå³ä½¿æ²¡æ£€æµ‹åˆ°å”¤é†’è¯ä¹Ÿç»§ç»­ç›‘å¬
        
        while rclpy.ok():
            if not self.listening:
                time.sleep(0.1)
                continue

            try:
                # è¯»å–éŸ³é¢‘æ•°æ®
                audio_data = self.stream.read(CHUNK_SIZE, exception_on_overflow=False)
                current_time = time.time()                
                # è®¡ç®—éŸ³é¢‘èƒ½é‡
                rms = self.calculate_rms(audio_data)

                # ---------- æ‰“æ–­æ£€æµ‹ ----------
                if self.tts_active:
                    # ç”¨åŒä¸€å‡½æ•°ä¼°ç®—åŠ¨æ€é˜ˆå€¼
                    dynamic_threshold_tts = self.update_background_energy(rms)
                    if rms > dynamic_threshold_tts:        # æœ‰äººæ’è¯
                        self.speech_counter += 1
                        need_frames = int(self.min_speech_duration * SAMPLE_RATE / CHUNK_SIZE)
                        if self.speech_counter >= need_frames:
                            # è§¦å‘æ‰“æ–­
                            interrupt_msg = Bool()
                            interrupt_msg.data = True
                            self.interrupt_pub.publish(interrupt_msg)
                            self.get_logger().info("ğŸ›‘ TTS æ’­æ”¾ä¸­æ£€æµ‹åˆ°ç”¨æˆ·è®²è¯ï¼Œå·²å‘é€æ‰“æ–­ä¿¡å·")
                            # å¤ä½è®¡æ•°å™¨ï¼Œé¿å…é¢‘ç¹
                            self.speech_counter = 0
                            self.silence_counter = 0
                    else:
                        self.speech_counter = 0

                    # æ‰“æ–­æ¨¡å¼ä¸‹è·³è¿‡åç»­ VAD & è¯†åˆ«é€»è¾‘
                    continue
                
                # æ›´æ–°åŠ¨æ€é˜ˆå€¼ (æ ¹æ®èƒŒæ™¯å™ªéŸ³è‡ªé€‚åº”)
                dynamic_threshold = self.update_background_energy(rms)
                
                # VAD çŠ¶æ€æœº
                if self.vad_state == "silence":
                    # å§‹ç»ˆä¿æŒå†å²ç¼“å†²åŒº
                    self.speech_buffer.append(audio_data)
                    
                    # æ£€æµ‹å£°éŸ³æ´»åŠ¨
                    if rms > dynamic_threshold:
                        self.speech_counter += 1
                        # è®°å½•RMSç”¨äºè°ƒè¯•
                        if self.speech_counter == 1:
                            self.get_logger().debug(f"æ£€æµ‹åˆ°æ½œåœ¨è¯­éŸ³å¼€å§‹ (RMS: {rms:.4f}, é˜ˆå€¼: {dynamic_threshold:.4f})")
                            
                        if self.speech_counter >= int(self.min_speech_duration * SAMPLE_RATE / CHUNK_SIZE):
                            # æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
                            self.vad_state = "speech"
                            self.speech_counter = 0
                            self.silence_counter = 0
                            self.last_speech_time = current_time
                            
                            # å°†å†å²ç¼“å†²åŒºæ·»åŠ åˆ°å½“å‰è¯­éŸ³
                            self.current_speech = bytearray()
                            for buffered_chunk in self.speech_buffer:
                                self.current_speech.extend(buffered_chunk)
                            
                            self.get_logger().info(f"ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ (RMS: {rms:.4f}, é˜ˆå€¼: {dynamic_threshold:.4f})")
                    else:
                        self.speech_counter = 0
                
                elif self.vad_state == "speech":
                    # æ·»åŠ éŸ³é¢‘åˆ°å½“å‰è¯­éŸ³ç¼“å†²åŒº
                    self.current_speech.extend(audio_data)
                    
                    if rms <= dynamic_threshold * 0.8:  # é™éŸ³é˜ˆå€¼ç•¥ä½äºåŠ¨æ€é˜ˆå€¼
                        self.silence_counter += 1
                        silence_duration = self.silence_counter * CHUNK_SIZE / SAMPLE_RATE
                        
                        if silence_duration >= self.silence_duration:
                            # æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
                            self.vad_state = "processing"
                            speech_duration = len(self.current_speech) / (SAMPLE_RATE * 2)  # 2 bytes per sample
                            self.get_logger().info(f"ğŸ”‡ æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ (æ—¶é•¿: {speech_duration:.2f}s)")
                            
                            # å¤„ç†è¯­éŸ³
                            if speech_duration > 0.5:  # ç¡®ä¿è¯­éŸ³ç‰‡æ®µè¶³å¤Ÿé•¿
                                # åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¤„ç†è¯­éŸ³ï¼Œé¿å…é˜»å¡ä¸»VADå¾ªç¯
                                self.transcribe_executor.submit(self.process_speech_chunk,
                                                                bytes(self.current_speech))
                            
                            # é‡ç½®çŠ¶æ€
                            self.current_speech = bytearray()
                            self.silence_counter = 0
                            self.vad_state = "silence"
                    else:
                        self.silence_counter = 0
                        self.last_speech_time = current_time

                # è¶…æ—¶ä¿æŠ¤ï¼šå¦‚æœè¯­éŸ³æŒç»­å¤ªä¹…ï¼Œå¼ºåˆ¶å¤„ç†
                if self.vad_state == "speech" and (current_time - self.last_speech_time) > 10.0:
                    self.get_logger().info("â° è¯­éŸ³è¶…æ—¶ï¼Œå¼ºåˆ¶å¤„ç†")
                    speech_data = bytes(self.current_speech)
                    self.transcribe_executor.submit(self.process_speech_chunk, speech_data)
                    
                    self.current_speech = bytearray()
                    self.vad_state = "silence"

            except Exception as e:
                self.get_logger().error(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
                time.sleep(0.05)

    def process_speech_chunk(self, speech_data):
        """å¤„ç†æ£€æµ‹åˆ°çš„è¯­éŸ³ç‰‡æ®µ"""
        if len(speech_data) < SAMPLE_RATE * 2 * 0.3:  # å°äº0.3ç§’çš„è¯­éŸ³å¿½ç•¥
            self.get_logger().debug("è¯­éŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œå¿½ç•¥")
            return

        try:
            # åˆ›å»ºä¸´æ—¶WAVæ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp:
                self.write_wav(tmp.name, speech_data)
                transcript = self.transcribe_file(tmp.name)
                if transcript:
                    self.process_recognized_text(transcript)
        except Exception as e:
            self.get_logger().error(f"è¯­éŸ³å¤„ç†é”™è¯¯: {e}")

    def write_wav(self, path: str, pcm_bytes: bytes):
        """å†™ WAV å·¥å…·"""
        with wave.open(path, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            wf.writeframes(pcm_bytes)

    @tenacity.retry(stop=tenacity.stop_after_attempt(3),
                    wait=tenacity.wait_exponential(multiplier=1, min=1, max=10))
    def transcribe_file(self, fname: str) -> str:
        """Whisper è°ƒç”¨ï¼Œå¸¦é‡è¯•"""
        try:
            with open(fname, "rb") as f:
                params = dict(
                    model=MODEL_NAME,
                    file=f,
                    prompt="The wake word is 'captain'.",
                    temperature=0,
                    response_format="text"
                )
                if self.language:           # éç©ºåˆ™æ˜¾å¼æŒ‡å®šè¯­è¨€
                    params['language'] = self.language
                response = self.openai_client.audio.transcriptions.create(**params)
            return response.strip() if isinstance(response, str) else response.strip()
        except Exception as e:
            self.get_logger().error(f"OpenAI API è°ƒç”¨å¤±è´¥: {e}")
            return ""

    def tts_status_callback(self, msg: Bool):
        """TTS æ’­æ”¾çŠ¶æ€å›è°ƒ"""
        # True = æ’­æ”¾ä¸­ï¼›False = æ’­æ”¾ç»“æŸ
        self.tts_active = msg.data
        if self.tts_active:
            self.get_logger().info("æ£€æµ‹åˆ° TTS æ­£åœ¨æ’­æ”¾ï¼Œå¼€å¯æ‰“æ–­ç›‘å¬")
        else:
            self.get_logger().info("TTS æ’­æ”¾ç»“æŸï¼Œæ¢å¤æ­£å¸¸è¯†åˆ«")

    def process_recognized_text(self, text):
        """å¤„ç†è¯†åˆ«çš„æ–‡æœ¬ï¼Œæ£€æŸ¥å”¤é†’è¯"""
        if not text:
            return
            
        lower_text = text.lower()
        match_found = False
        matching_word = None
        
        # æ›´ä¸¥æ ¼çš„å”¤é†’è¯æ£€æŸ¥
        for word in self.wake_words:
            if word in lower_text:
                match_found = True
                matching_word = word
                break
                
            # æ¨¡ç³ŠåŒ¹é…ï¼Œå°¤å…¶é’ˆå¯¹å¥å­å¼€å¤´
            # å¦‚æœå¥å­ä»¥å”¤é†’è¯çš„éƒ¨åˆ†å¼€å¤´ï¼Œå¢åŠ åŒ¹é…å¯èƒ½æ€§
            if lower_text.startswith(word[:5]):  # æ£€æŸ¥æ˜¯å¦ä»¥å”¤é†’è¯å‰5ä¸ªå­—ç¬¦å¼€å¤´
                ratio = difflib.SequenceMatcher(None, lower_text[:len(word)], word).ratio()
                if ratio > 0.7:  # å¯¹å¥é¦–æ›´å®½æ¾çš„é˜ˆå€¼
                    match_found = True
                    matching_word = word
                    break
                    
            # ä¸€èˆ¬æ¨¡ç³ŠåŒ¹é…
            ratio = difflib.SequenceMatcher(None, lower_text, word).ratio()
            if ratio > 0.75:
                match_found = True
                matching_word = word
                break
                
        if match_found:
            msg = String()
            msg.data = text
            self.publisher_.publish(msg)
            self.get_logger().info(f"ğŸ”¥ è¯†åˆ«ç»“æœ (å”¤é†’è¯æ¿€æ´» '{matching_word}'): {text}")
        else:
            self.get_logger().info(f"æœªæ£€æµ‹åˆ°å”¤é†’è¯: {text}")

    def destroy_node(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()
        if hasattr(self, 'audio'):
            self.audio.terminate()
        super().destroy_node()

def main(args=None):
    rclpy.init(args=args)
    node = OpenAISTTNodeWithVAD()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        if node:
            node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()