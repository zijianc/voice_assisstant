#!/usr/bin/env python3
"""
Baseline Performance Test for Conventional STT â†’ LLM â†’ TTS Pipeline
================================================================

This script tests                # ç­‰å¾…æµ‹è¯•å®Œæˆ
                timeout = 120  # 120ç§’è¶…æ—¶ï¼ˆbaseline pipelineéœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                start_wait = time.time()
                while len(self.results) <= i and (time.time() - start_wait) < timeout:
                    rclpy.spin_once(self, timeout_sec=0.1)conventional pipeline using:
- openai_stt_node (STT)
- llm_node (LLM processing)  
- openai_tts_node (TTS)

Measures end-to-end latency and compares with OpenAI Realtime approach.

Usage:
    python3 baseline_performance_test.py --samples 30 --output baseline_results.json
"""

import argparse
import json
import csv
import time
import statistics
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import subprocess
import threading
import queue
import sys
import os
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool

class BaselineLatencyTest(Node):
    def __init__(self, num_samples=30):
        super().__init__('baseline_latency_test')
        self.num_samples = num_samples
        self.results = []
        self.current_test = {}
        self.test_queue = queue.Queue()
        self.measurement_lock = threading.Lock()
        
        # Test prompts (same as rigorous test for consistency)
        self.test_prompts = [
            "new way 4, what's the weather like today?",
            "new way 4, when does the next shuttle arrive?", 
            "new way 4, show me the campus dining options",
            "new way 4, what time does the library close?",
            "new way 4, how do I get to the engineering building?",
            "new way 4, what events are happening on campus?",
            "new way 4, where can I find available parking?",
            "new way 4, what's the shuttle schedule for tomorrow?",
            "new way 4, tell me about study spaces",
            "new way 4, how late is the cafeteria open?",
            "new way 4, what's the current campus occupancy?",
            "new way 4, show me the quickest route to the library",
            "new way 4, what health services are available?",
            "new way 4, when is the next campus tour?",
            "new way 4, where are the computer labs located?",
        ]
        
        # Publishers and subscribers for conventional pipeline
        self.speech_pub = self.create_publisher(String, 'speech_text', 10)
        self.llm_response_sub = self.create_subscription(
            String, 'llm_response', self.llm_response_callback, 10)
        self.tts_status_sub = self.create_subscription(
            Bool, 'tts_status', self.tts_status_callback, 10)
        self.llm_response_full_sub = self.create_subscription(
            String, 'llm_response_full', self.llm_response_full_callback, 10)
        
        # Timing variables
        self.test_start_time = None
        self.first_response_time = None
        self.tts_start_time = None
        self.tts_end_time = None
        self.current_test = {}
        self.current_test_id = 0
        self.test_completed = False
        
        self.get_logger().info('Baseline Latency Test Node initialized')

    def speech_callback(self, msg):
        """è®°å½•è¯­éŸ³è¾“å…¥æ—¶é—´"""
        if self.test_start_time is None:
            self.test_start_time = time.time()
            self.get_logger().info(f'Speech input detected: {msg.data[:50]}...')

    def llm_response_callback(self, msg):
        """è®°å½•ç¬¬ä¸€ä¸ªLLMå“åº”æ—¶é—´"""
        # éªŒè¯è¿™ä¸ªå“åº”æ˜¯å¦å±äºå½“å‰æµ‹è¯•
        if self.test_completed or self.test_start_time is None:
            self.get_logger().warning(f'Ignoring late/invalid LLM response for test {self.current_test_id}: {msg.data[:30]}...')
            return
            
        # æ£€æŸ¥å“åº”æ—¶é—´æ˜¯å¦åˆç†ï¼ˆä¸åº”è¯¥å¤ªå¿«ï¼‰
        if self.test_start_time is not None:
            response_time = (time.time() - self.test_start_time) * 1000
            if response_time < 100:  # å°‘äº100msçš„å“åº”ä¸åˆç†
                self.get_logger().warning(f'Suspiciously fast LLM response ({response_time:.1f}ms), ignoring: {msg.data[:30]}...')
                return
            
        if self.first_response_time is None:
            self.first_response_time = time.time()
            self.get_logger().info(f'First LLM response: {msg.data[:50]}...')

    def llm_response_full_callback(self, msg):
        """è®°å½•å®Œæ•´LLMå“åº”"""
        if self.test_start_time is not None:
            self.get_logger().info('Full LLM response received')

    def tts_status_callback(self, msg):
        """è®°å½•TTSçŠ¶æ€å˜åŒ–"""
        # éªŒè¯è¿™ä¸ªäº‹ä»¶æ˜¯å¦å±äºå½“å‰æµ‹è¯•
        if self.test_completed or self.test_start_time is None:
            self.get_logger().warning(f'Ignoring late/invalid TTS event for test {self.current_test_id}')
            return
            
        current_time = time.time()
        if msg.data:  # TTS started
            # æ£€æŸ¥TTSå¼€å§‹æ—¶é—´æ˜¯å¦åˆç†ï¼ˆä¸åº”è¯¥å¤ªå¿«ï¼‰
            if self.test_start_time is not None:
                tts_start_time = (current_time - self.test_start_time) * 1000
                if tts_start_time < 100:  # å°‘äº100msçš„TTSå¼€å§‹ä¸åˆç†
                    self.get_logger().warning(f'Suspiciously fast TTS start ({tts_start_time:.1f}ms), ignoring')
                    return
                    
            if self.tts_start_time is None:
                self.tts_start_time = current_time
                self.get_logger().info('TTS playback started')
        else:  # TTS ended
            if self.tts_end_time is None and self.tts_start_time is not None:
                self.tts_end_time = current_time
                self.get_logger().info('TTS playback ended')
                self.complete_measurement()
                self.test_completed = True

    def complete_measurement(self):
        """å®Œæˆä¸€æ¬¡æµ‹é‡å¹¶è®°å½•ç»“æœ"""
        with self.measurement_lock:
            if self.test_start_time is None:
                return
                
            try:
                # è®¡ç®—å„ç§å»¶è¿ŸæŒ‡æ ‡
                speech_to_first_response = None
                speech_to_tts_start = None
                total_response_time = None
                
                if self.first_response_time:
                    speech_to_first_response = (self.first_response_time - self.test_start_time) * 1000
                    
                if self.tts_start_time:
                    speech_to_tts_start = (self.tts_start_time - self.test_start_time) * 1000
                    
                if self.tts_end_time:
                    total_response_time = (self.tts_end_time - self.test_start_time) * 1000

                measurement = {
                    'test_id': len(self.results) + 1,
                    'timestamp': datetime.now().isoformat(),
                    'speech_to_first_response_ms': speech_to_first_response,
                    'speech_to_tts_start_ms': speech_to_tts_start,
                    'total_response_time_ms': total_response_time,
                    'prompt': self.current_test.get('prompt', 'unknown'),
                    'success': True,
                    'timeout': False
                }
                
                self.results.append(measurement)
                self.get_logger().info(f'Measurement {len(self.results)} completed:')
                
                # Safe formatting - only show values that are not None
                if speech_to_first_response is not None:
                    self.get_logger().info(f'  Speechâ†’First Response: {speech_to_first_response:.1f}ms')
                else:
                    self.get_logger().info(f'  Speechâ†’First Response: Not measured')
                    
                if speech_to_tts_start is not None:
                    self.get_logger().info(f'  Speechâ†’TTS Start: {speech_to_tts_start:.1f}ms')
                else:
                    self.get_logger().info(f'  Speechâ†’TTS Start: Not measured')
                    
                if total_response_time is not None:
                    self.get_logger().info(f'  Total Response: {total_response_time:.1f}ms')
                else:
                    self.get_logger().info(f'  Total Response: Not measured')
                
            except Exception as e:
                self.get_logger().error(f'Error completing measurement: {e}')
            finally:
                self.reset_measurement()

    def reset_measurement(self):
        """é‡ç½®æµ‹é‡å˜é‡"""
        self.test_start_time = None
        self.first_response_time = None
        self.tts_start_time = None
        self.tts_end_time = None
        self.current_test = {}
        self.test_completed = False

    def run_test_sequence(self):
        """è¿è¡Œæµ‹è¯•åºåˆ—"""
        self.get_logger().info(f'Starting baseline test with {self.num_samples} samples')
        
        for i in range(self.num_samples):
            try:
                # å¼ºåˆ¶ç­‰å¾…ï¼Œç¡®ä¿ä¸Šä¸€ä¸ªæµ‹è¯•çš„æ‰€æœ‰å¼‚æ­¥æ“ä½œå®Œæˆ
                if i > 0:
                    self.get_logger().info(f'Waiting for previous test cleanup...')
                    time.sleep(5.0)  # ç®€å•ç­‰å¾…ï¼Œé¿å…å¤æ‚çš„ROS2 spinæ“ä½œ
                
                # é€‰æ‹©æµ‹è¯•æç¤ºè¯
                prompt = self.test_prompts[i % len(self.test_prompts)]
                self.current_test = {'prompt': prompt}
                self.current_test_id = i + 1
                
                self.get_logger().info(f'Test {i+1}/{self.num_samples}: {prompt[:50]}...')
                
                # é‡ç½®æµ‹é‡çŠ¶æ€
                self.reset_measurement()
                
                # å‘é€è¯­éŸ³è¾“å…¥
                self.test_start_time = time.time()
                msg = String()
                msg.data = prompt
                self.speech_pub.publish(msg)
                
                # ç­‰å¾…æµ‹é‡å®Œæˆæˆ–è¶…æ—¶
                timeout = 120  # 120ç§’è¶…æ—¶ï¼ˆbaseline pipelineéœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                start_wait = time.time()
                while not self.test_completed and (time.time() - start_wait) < timeout:
                    rclpy.spin_once(self, timeout_sec=0.1)
                
                if not self.test_completed:
                    self.get_logger().warning(f'Test {i+1} timed out after {timeout}s')
                    self.results.append({
                        'test_id': i + 1,
                        'timestamp': datetime.now().isoformat(),
                        'speech_to_first_response_ms': None,
                        'speech_to_tts_start_ms': None,
                        'total_response_time_ms': None,
                        'prompt': prompt,
                        'success': False,
                        'timeout': True
                    })
                    self.test_completed = True
                
                # æµ‹è¯•é—´éš”
                time.sleep(2.0)
                
            except KeyboardInterrupt:
                self.get_logger().info('Test interrupted by user')
                break
            except Exception as e:
                self.get_logger().error(f'Error in test {i+1}: {e}')
                continue

    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ç»“æœ"""
        if not self.results:
            return {}
            
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_results = [r for r in self.results if not r.get('timeout', False)]
        
        stats = {
            'total_samples': len(self.results),
            'valid_samples': len(valid_results),
            'timeout_count': len(self.results) - len(valid_results)
        }
        
        if not valid_results:
            return stats
            
        # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„ç»Ÿè®¡æ•°æ®
        metrics = ['speech_to_first_response_ms', 'speech_to_tts_start_ms', 'total_response_time_ms']
        
        for metric in metrics:
            values = [r[metric] for r in valid_results if r[metric] is not None]
            if values:
                stats[metric] = {
                    'count': len(values),
                    'mean': statistics.mean(values),
                    'median': statistics.median(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0,
                    'min': min(values),
                    'max': max(values),
                    'p95': np.percentile(values, 95) if len(values) > 1 else values[0],
                    'confidence_interval_95': self.calculate_ci(values, 0.95) if len(values) > 10 else None
                }
        
        return stats

    def calculate_ci(self, data, confidence=0.95):
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        if len(data) < 2:
            return None
            
        n = len(data)
        mean = statistics.mean(data)
        std_err = statistics.stdev(data) / (n ** 0.5)
        
        # ä½¿ç”¨tåˆ†å¸ƒï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        t_value = 2.0 if n > 30 else 2.5  # ç®€åŒ–çš„tå€¼
        margin = t_value * std_err
        
        return [mean - margin, mean + margin]

    def save_results(self, output_file):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        stats = self.calculate_statistics()
        
        output_data = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'test_type': 'baseline_conventional_pipeline',
                'architecture': 'STT â†’ LLM â†’ TTS',
                'num_samples': self.num_samples,
                'description': 'Baseline performance test for conventional STTâ†’LLMâ†’TTS pipeline'
            },
            'statistics': stats,
            'raw_results': self.results
        }
        
        # ä¿å­˜JSONç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVç»“æœ
        csv_file = output_file.replace('.json', '.csv')
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            if self.results:
                # å®šä¹‰å›ºå®šçš„å­—æ®µåï¼Œç¡®ä¿æ‰€æœ‰è®°å½•éƒ½æœ‰ç›¸åŒå­—æ®µ
                fieldnames = [
                    'test_id', 'timestamp', 'speech_to_first_response_ms', 
                    'speech_to_tts_start_ms', 'total_response_time_ms', 
                    'prompt', 'success', 'timeout'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰æ‰€æœ‰å­—æ®µ
                for result in self.results:
                    clean_result = {}
                    for field in fieldnames:
                        clean_result[field] = result.get(field, None)
                    writer.writerow(clean_result)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(output_file.replace('.json', '_report.txt'), stats)
        
        self.get_logger().info(f'Results saved to {output_file}')
        self.get_logger().info(f'CSV data saved to {csv_file}')

    def generate_report(self, report_file, stats):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Baseline Performance Test Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Architecture: Conventional STT â†’ LLM â†’ TTS Pipeline\n")
            f.write(f"Total Samples: {stats.get('total_samples', 0)}\n")
            f.write(f"Valid Samples: {stats.get('valid_samples', 0)}\n")
            f.write(f"Timeouts: {stats.get('timeout_count', 0)}\n\n")
            
            # è¯¦ç»†ç»Ÿè®¡
            metrics = {
                'speech_to_first_response_ms': 'Speech â†’ First Response (ms)',
                'speech_to_tts_start_ms': 'Speech â†’ TTS Start (ms)', 
                'total_response_time_ms': 'Total Response Time (ms)'
            }
            
            for metric_key, metric_name in metrics.items():
                if metric_key in stats:
                    metric_stats = stats[metric_key]
                    f.write(f"{metric_name}:\n")
                    f.write(f"  Count: {metric_stats.get('count', 0)}\n")
                    f.write(f"  Median: {metric_stats.get('median', 0):.1f}ms\n")
                    f.write(f"  Mean: {metric_stats.get('mean', 0):.1f}ms\n")
                    f.write(f"  Std Dev: {metric_stats.get('std', 0):.1f}ms\n")
                    f.write(f"  Min: {metric_stats.get('min', 0):.1f}ms\n")
                    f.write(f"  Max: {metric_stats.get('max', 0):.1f}ms\n")
                    f.write(f"  P95: {metric_stats.get('p95', 0):.1f}ms\n")
                    if metric_stats.get('confidence_interval_95'):
                        ci = metric_stats['confidence_interval_95']
                        f.write(f"  95% CI: [{ci[0]:.1f}, {ci[1]:.1f}]ms\n")
                    f.write("\n")


def check_prerequisites():
    """æ£€æŸ¥æµ‹è¯•å‰ææ¡ä»¶"""
    print("ğŸ” Checking prerequisites...")
    
    # æ£€æŸ¥ROS2ç¯å¢ƒ
    if not os.path.exists('/opt/ros/humble/setup.bash'):
        raise Exception("ROS2 Humble not found. Please install ROS2 Humble.")
    
    # æ£€æŸ¥å·¥ä½œç©ºé—´
    if not os.path.exists('/workspaces/ros2_ws/install/setup.bash'):
        raise Exception("ROS2 workspace not built. Please run 'colcon build'.")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.environ.get('OPENAI_API_KEY'):
        raise Exception("OPENAI_API_KEY not set. Please set in .env file.")
    
    print("âœ… Prerequisites check passed")


def start_baseline_nodes():
    """å¯åŠ¨baselineèŠ‚ç‚¹"""
    print("ğŸš€ Starting baseline pipeline nodes...")
    
    nodes_to_start = [
        'ros2 run my_voice_assistant openai_stt_node',
        'ros2 run my_voice_assistant llm_node', 
        'ros2 run my_voice_assistant openai_tts_node'
    ]
    
    processes = []
    for cmd in nodes_to_start:
        try:
            print(f"Starting: {cmd}")
            proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            processes.append(proc)
            time.sleep(2)  # ç»™èŠ‚ç‚¹å¯åŠ¨æ—¶é—´
        except Exception as e:
            print(f"Failed to start {cmd}: {e}")
    
    print("âœ… Baseline nodes started")
    return processes


def main():
    parser = argparse.ArgumentParser(description='Baseline Performance Test')
    parser.add_argument('--samples', type=int, default=30, help='Number of test samples')
    parser.add_argument('--output', type=str, default='baseline_results.json', help='Output file')
    parser.add_argument('--no-nodes', action='store_true', help='Skip starting nodes (assume already running)')
    
    args = parser.parse_args()
    
    try:
        # æ£€æŸ¥å‰ææ¡ä»¶
        check_prerequisites()
        
        # å¯åŠ¨ROS2
        rclpy.init()
        
        # å¯åŠ¨baselineèŠ‚ç‚¹ï¼ˆé™¤éæŒ‡å®šè·³è¿‡ï¼‰
        processes = []
        if not args.no_nodes:
            processes = start_baseline_nodes()
            time.sleep(5)  # ç­‰å¾…èŠ‚ç‚¹å®Œå…¨å¯åŠ¨
        
        # åˆ›å»ºæµ‹è¯•èŠ‚ç‚¹
        test_node = BaselineLatencyTest(num_samples=args.samples)
        
        print(f"ğŸ¯ Starting baseline performance test with {args.samples} samples...")
        print("Press Ctrl+C to stop early")
        
        # è¿è¡Œæµ‹è¯•
        test_node.run_test_sequence()
        
        # ä¿å­˜ç»“æœ
        test_node.save_results(args.output)
        
        # æ‰“å°æ±‡æ€»
        stats = test_node.calculate_statistics()
        print("\nğŸ“Š Test Summary:")
        print(f"Valid samples: {stats.get('valid_samples', 0)}/{stats.get('total_samples', 0)}")
        
        if 'speech_to_first_response_ms' in stats:
            median = stats['speech_to_first_response_ms'].get('median', 0)
            print(f"Speech â†’ First Response: {median:.1f}ms (median)")
        
        if 'total_response_time_ms' in stats:
            median = stats['total_response_time_ms'].get('median', 0) 
            print(f"Total Response Time: {median:.1f}ms (median)")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Test stopped by user")
    except Exception as e:
        print(f"âŒ Error: {e}")
    finally:
        # æ¸…ç†
        rclpy.shutdown()
        
        # åœæ­¢å¯åŠ¨çš„èŠ‚ç‚¹
        for proc in processes:
            try:
                proc.terminate()
                proc.wait(timeout=5)
            except:
                proc.kill()


if __name__ == '__main__':
    main()